{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5af6b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "import os\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "CMD = \"curl -v minio-lake:9000 2>&1 | grep -o '(.*).' | tr -d '() '\"\n",
    "# Capture the output of the command using subprocess\n",
    "MINIO_END_POINT = 'http://' + os.popen(CMD).read().replace('\\n', '')  +':9000'\n",
    "\n",
    "# Configure Spark with necessary packages and Iceberg/Nessie settings\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('sales_data_app')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', MINIO_END_POINT)\n",
    ")\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "# spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e187d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a schema for the sales data\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"order_date\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create a DataFrame with messy sales data (including duplicates and errors)\n",
    "sales_data = [\n",
    "    (1, 101, \"Laptop\", 1, 1000.00, \"2023-08-01\"),\n",
    "    (2, 102, \"Mouse\", 2, 25.50, \"2023-08-01\"),\n",
    "    (3, 103, \"Keyboard\", 1, 45.00, \"2023-08-01\"),\n",
    "    (1, 101, \"Laptop\", 1, 1000.00, \"2023-08-01\"),  # Duplicate\n",
    "    (4, 104, \"Monitor\", None, 200.00, \"2023-08-02\"),  # Missing quantity\n",
    "    (5, None, \"Mouse\", 1, 25.50, \"2023-08-02\")  # Missing customer_id\n",
    "]\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, schema)\n",
    "\n",
    "# Create the \"sales\" namespace\n",
    "spark.sql(\"CREATE NAMESPACE nessie.sales;\").show()\n",
    "\n",
    "# Write the DataFrame to an Iceberg table in the Nessie catalog\n",
    "# NOTE this would start writing data to MINIO (S3) before Nessie. So if nessie namespace doesn't existed, spark will double write data onto MINIO.\n",
    "sales_df.writeTo(\"nessie.sales.sales_data_abc\").createOrReplace()\n",
    "\n",
    "# Verify by reading from the Iceberg table\n",
    "spark.read.table(\"nessie.sales.sales_data_abc\").show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540707bf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "\n",
    "MINIO_ACCESS_KEY = os.environ.get('MINIO_ACCESS_KEY')\n",
    "MINIO_SECRET_KEY = os.environ.get('MINIO_SECRET_KEY')\n",
    "\n",
    "# Define Minio connection parameters\n",
    "minio_client = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=MINIO_END_POINT,  # Minio IP address from docker inspect\n",
    "    aws_access_key_id=MINIO_ACCESS_KEY,\n",
    "    aws_secret_access_key=MINIO_SECRET_KEY,\n",
    "    region_name='ap-southeast-1'\n",
    ")\n",
    "\n",
    "# Specify the bucket and metadata file path\n",
    "bucket_name = 'datalakehouse'\n",
    "metadata_file_key = 'sales/sales_data_raw_7720cf28-1423-431e-83ab-587e1da2ed48/metadata/00000-d50b2fc1-a0b9-468d-b01b-49b3ed5d1ad4.metadata.json'  # Example metadata path\n",
    "\n",
    "# Download the metadata file\n",
    "metadata_file = minio_client.get_object(Bucket=bucket_name, Key=metadata_file_key)\n",
    "metadata_content = metadata_file['Body'].read().decode('utf-8')\n",
    "\n",
    "# Parse and print the metadata content\n",
    "metadata_json = json.loads(metadata_content)\n",
    "print(json.dumps(metadata_json, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc69d36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error\n",
    "import json\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "CMD = \"curl -v minio-lake:9000 2>&1 | grep -o '(.*).' | tr -d '() '\"\n",
    "# Capture the output of the command using subprocess\n",
    "MINIO_END_POINT = os.popen(CMD).read().replace('\\n', '')  +':9000'\n",
    "MINIO_ACCESS_KEY = os.environ.get('MINIO_ACCESS_KEY')\n",
    "MINIO_SECRET_KEY = os.environ.get('MINIO_SECRET_KEY')\n",
    "\n",
    "# Define Minio connection parameters\n",
    "minio_client = Minio(MINIO_END_POINT,\n",
    "    access_key=MINIO_ACCESS_KEY,\n",
    "    secret_key=MINIO_SECRET_KEY,\n",
    "    secure=False\n",
    ")\n",
    "\n",
    "# Specify the bucket and metadata file path\n",
    "bucket_name = 'datalakehouse'\n",
    "metadata_file_key = 'sales/sales_data_abc_6f787c15-8d54-44e4-b4b7-22e11cbda9e2/metadata/00000-731e5735-bd02-44b0-a011-354164361fa9.metadata.json'  # Example metadata path\n",
    "\n",
    "# Download the metadata file\n",
    "metadata_file = minio_client.get_object(bucket_name, metadata_file_key)\n",
    "metadata_content = metadata_file.read().decode('utf-8')\n",
    "\n",
    "# Parse and print the metadata content\n",
    "metadata_json = json.loads(metadata_content)\n",
    "print(json.dumps(metadata_json, indent=4))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
