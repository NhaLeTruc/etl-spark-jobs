# spark configs
spark.master                            spark://spark-master:7077
spark.jars                              /opt/bitnami/spark/jars/*.jar
# spark.jars.packages                     org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.3,\
#                                         org.apache.iceberg:iceberg-aws-bundle:1.4.3,\                        
#                                         org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.78.0,\
#                                         org.postgresql:postgresql:42.7.7,\                                     
# spark.jars.repositories                 https://mvnrepository.com/artifact,https://repo1.maven.org/maven2,https://repository.apache.org/content/groups/public/,https://oss.sonatype.org/content/repositories/snapshots,https://maven-central.storage-download.googleapis.com/maven2/,https://mmlspark.azureedge.net/maven

# minio
spark.hadoop.fs.s3a.path.style.access   true
spark.hadoop.fs.s3a.impl                org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.endpoint.region     ap-southeast-1
aws.region                              ap-southeast-1
# These credentials could be restrieved through ENV
# spark.hadoop.fs.s3a.access.key          minioadmin
# spark.hadoop.fs.s3a.secret.key          minioadmin
# spark.hadoop.fs.s3a.endpoint            http://minio-lake:9000

# spark logs
spark.eventLog.enabled                  true
spark.eventLog.dir                      /tmp/spark-events
spark.history.fs.logDirectory           /tmp/spark-events
# spark.sql.debug.maxToStringFields       75

# Iceberg and Nessie
spark.sql.extensions                     org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions
spark.sql.catalog.nessie                 org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.nessie.catalog-impl    org.apache.iceberg.nessie.NessieCatalog
spark.sql.catalog.nessie.uri             http://nessie:19120/api/v1
spark.sql.catalog.nessie.ref             main
spark.sql.defaultCatalog                 nessie
spark.sql.catalog.nessie.authentication.type  NONE
# Set Minio as the S3 endpoint for Iceberg storage
spark.sql.catalog.nessie.warehouse       s3a://lakehouse
# spark.sql.catalog.nessie.s3.endpoint	 http://minio-lake:9000 # This credential could be restrieved through ENV
spark.sql.catalog.nessie.io-impl		 org.apache.iceberg.aws.s3.S3FileIO

# spark.sql.catalog.nessie.authentication.type  BEARER
# spark.sql.catalog.nessie.authentication.token XXXXX-XXXXX-XXXXX

# These are for different types of catalog: HIVE
# spark.sql.catalogImplementation        in-memory
# spark.sql.catalogImplementation        hive

# ======================================================================================
# Performance Tuning Configurations
# ======================================================================================
#
spark.sql.execution.pyarrow.enabled    true
spark.sql.adaptive.enabled             true
spark.sql.files.maxRecordsPerFile      1000000
spark.sql.files.maxPartitionBytes      256MB
spark.sql.parquet.fs.optimized.committer.optimization-enabled true

# These settings are configured to maximize resource utilization for faster processing.
#
# -- Resource Allocation ---------------------------------------------------------------
# Use dynamic allocation to let Spark scale executors based on workload.
spark.dynamicAllocation.enabled                 true
spark.dynamicAllocation.shuffleTracking.enabled true
spark.dynamicAllocation.minExecutors            1
spark.dynamicAllocation.maxExecutors            10
spark.dynamicAllocation.initialExecutors        2

# -- General Execution -----------------------------------------------------------------
spark.executor.cores                4
spark.cores.max                     12

# -- Memory Settings -------------------------------------------------------------------
spark.driver.memory                 12g
spark.executor.memory               12g
spark.executor.memoryOverhead       1g


